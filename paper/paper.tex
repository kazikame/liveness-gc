\documentclass[9pt]{sigplanconf}
\usepackage{etex}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage{booktabs}
\usepackage{boxedminipage}
\usepackage[T1]{fontenc}
%\usepackage[pdf]{pstricks}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{url}
\usepackage[normalem]{ulem}
\usepackage{schemepgm}
\usepackage{graphicx}
\usepackage{graphpap}
\usepackage{tabularx}
\usepackage [plain,noend,noline,boxed]{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{pstricks}
\usepackage{pst-text}
\usepackage{pst-node}
\usepackage{pst-tree}
\usepackage{pst-rel-points}
\usepackage{bcprules}
\usepackage{subfigure}
%\usepackage{program}
%\usepackage{algorithmic}
\usepackage[boxed]{algorithm2e}


\input{fh_defs}
\def\drawplusplus#1#2#3{\hbox to 0pt{\hbox to #1{\hfill\vrule height #3 depth
      0pt width #2\hfill\vrule height #3 depth 0pt width #2\hfill
      }}\vbox to #3{\vfill\hrule height #2 depth 0pt width
      #1 \vfill}}
      %Poor man's typography
\def\concat{\mathrel{\drawplusplus {12pt}{0.4pt}{5pt}}}
      %It would be better to specify these in font-relative measures, but it
      %probably doesn't scale anyway.
\newcommand{\mycomment}[1]{}
\definecolor{Myblue}{rgb}{.2,0,1}
\newcommand{\comment}[1]{{\color{Myblue}{(#1)}}}
\newcommand{\cred}[1]{{\color{red}{#1}}}
\newcommand{\blankout}[1]{}


%\newcommand{\deltacall}[3]{\delta_{#1}({#2},{#3})}
\newcommand{\scmin} {\mbox{\sf\em in}}
\newcommand{\scmnull}{\mbox{\sf\em null?}}
\newcommand{\scmpair}{\mbox{\sf\em pair?}}
\newcommand{\scmprim}{\ensuremath{\mathsf{+}}}
\newcommand{\sh}[1]{{\colorbox{gray!20}{\framebox{$#1$}}}}
\def\myvec{\mathaccent"017E } 
\newcommand{\stk}{\mbox{S}}   
\newcommand{\ID}{\mbox{$\mathbf{ id}$}}
\newcommand{\bang}{\mbox{\sc bang}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\begin{document}
% --- Author Metadata here ---
%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be
%over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data
%(0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Liveness-Based Garbage Collection for Lazy Languages}

% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.
\cmt{{
\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors
section.
%
\author{
% 1st. author
\alignauthor K. Prasanna Kumar \\
       \affaddr{IIT Bombay,}\\
       \affaddr{Mumbai 400076, India}
       \email{prasannak@cse.iitb.ac.in}
% 2nd. author
\alignauthor Amey Karkare\\
       \affaddr{IIT Kanpur}\\
       \affaddr{Kanpur 208016, India}
       \email{karkare@cse.iitk.ac.in}
% 3rd. author
\alignauthor Amitabha Sanyal \\
       \affaddr{IIT Bombay}\\
       \affaddr{Mumbai 400076, India}\\
       \email{as@cse.iitb.ac.in}
}
}}
\authorinfo{Double Blind Review}{}{}
\maketitle



\begin{abstract}
We consider  the problem of reducing  the memory required to  run lazy
first-order functional programs.  To do  this, we analyze programs for
{\em liveness} of heap allocated data, i.e.  data that can possibly be
used in the future.  The accompanying garbage collector is modified to
use the  result of the analysis  to preserve only {\em  live} data---a
subset  of {\em  reachable  data}---during  garbage collection.   This
results  in an  increase  in the  garbage  reclaimed and  consequently
reduces the memory  footprints of programs.  While  this technique has
already been shown to yield  benefits for eager first-order languages,
the presence of  closures at run-time poses  additional challenges for
lazy languages.  Closures require changes both in the earlier liveness
analysis for heap allocated memory  and also in the garbage collection
scheme required to reclaim memory that is not live.

Our  analysis  is a  context-sensitive  analysis  that annotates  each
potential  garbage collection  point  in  the program  with  a set  of
deterministic finite-state  automata (DFA) describing the  liveness at
that  point.   We  also  present a  practical  implementation  of  our
analysis in  a garbage collector.   The collector, when  invoked, uses
the DFA set to preserve live  objects.  As a result, fewer objects are
marked and preserved when compared  to a reachability based marking.
Our experiments  confirm that liveness-based collection  increases the
garbage reclaimed,  consequently decreasing the number  of collections
on many  practical programs.  On  our benchmark programs, we  obtain a
decrease of  up to  y\% in  the number  of collections,  and up  to x\%
decrease in overall garbage collection time.
\end{abstract}

\category{D.3.4}{Programming Languages}{Processors}[Memory Management
  (Garbage Collection), Optimizations]
\category{F.3.2}{Logic and Meanings Of Programs}{Semantics of
  Programming Languages}[Program Analysis]

\terms{Algorithms, Languages, Theory}

\keywords{Heap Analysis, Memory Management, Garbage Collection}

\section{Introduction}
\label{sec:intro}

Functional programs extensively use dynamically allocated memory.  The
allocation  is either  explicit (for  example, using  constructors) or
implicit  (during creation  of  closures).  Programs  written in  lazy
functional languages put additional  demands on memory as they require
closures to  be carried  from the  point of creation  to the  point of
evaluation.

While  the runtime  system  of most  functional  languages includes  a
garbage  collector  (GC)  to  efficiently  reclaim  memory,  empirical
studies on Scheme~\cite{karkare06effectiveness} and, more importantly,
on Haskell~\cite{rojemo96lag}
programs have shown that  GCs leave uncollected a large
number of  memory objects that are  reachable but not  live (here {\em
  live} means the  object can potentially be used by  the program at a
later stage).  This results in unnecessary memory retention.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%% MOTIVATING EXAMPLE
%% \newcommand{\nilfigure}
%% {\scalebox{0.75}{
%% \psset{unit=1mm,nodesep=0mm,labelsep=0.5mm}
%% \begin{pspicture}(0,0)(1,1)
%% %\psgrid[xunit=1cm,yunit=1cm,gridwidth=.2pt,subgridwidth=.1pt,
%% subgriddiv=5,subgridcolor=gray,gridcolor=blue](0,0)(1,1)
%% \putnode{start}{origin}{0}{0}{}
%% \putnode{stop}{origin}{10}{10}{}
%% \ncline[offsetB=0,nodesepB=0,linewidth=.7]{-}{start}{stop} %here
%% \end{pspicture}
%% }}

\begin{figure}[t!]
  \psset{unit=1mm}
  \begin{pspicture}(90,48)(0,-48)
    %\psframe(90,48)(0,-48)
    \begin{tabular}{@{}c@{}}
      \psframebox{\sf
	\renewcommand{\arraystretch}{1}{
	  \begin{uprogram}
            \UFL\
            \UNL{0} (\DEFINE\ (\length\  \pl)
	    \UNL{1}  (\SIF~(\NULLQ \ \pl) $0$
            (\PRIM\ 1\ (\CAR\  \pl))))
            \UNL{0}
	    \UNL{0}  (\DEFINE\ (\append\  \lista\ \listb)
	    \UNL{1}  (\SIF~(\NULLQ \ \lista)
	    \listb
	    \UNL{2} \hspace*{-0.1cm}(\CONS\ (\CAR\  \lista) (\append\
            (\CDR\  \lista)
            \listb))))
            \UNL{0}
            \UNL{0} (\LET\ \px\
            $\leftarrow$\ (\CONS\ $5$
            (\CONS\ (\CONS\ $6$ \NIL) \NIL) \IN
	    \UNL{1} (\LET\ \py\   $\leftarrow$\  (\CONS\ $3$ \NIL) \IN
	    \UNL{2}
            (\LET\ \pz\  $\leftarrow$\  (\append\ \px\  \py)\ \IN\
            \UNL{3} (\SIF~(\NULLQ~(\CAR~(\CDR~\pz)))~$0$~$\pi$:\
            (\length\ \pz))))))
	  \end{uprogram}
      }}
      \\ 
      (a) Example program. \\  \\
%      \includegraphics[width=.45\textwidth]{motiv-example}
      \input{mem-graph} \\ 
      \renewcommand{\arraystretch}{.9}
      \begin{tabular}[t]{l}
        (b) Memory graph at $\pi$. Thick edges denote live
        links. Traversal \\{\white (b)} stops at edges
        marked $\times$ during garbage collection.
      \end{tabular}
    \end{tabular} 
  \end{pspicture}
  \vspace*{-3ex}
  \caption{Example Program and its Memory Graph}\label{fig:mot-example}
\end{figure}

In this paper,  we propose the use of liveness  analysis of heap cells
for garbage collection in a lazy first-order functional language.  The
central notion in our analysis  is a generalization of liveness called
{\em  demand}---the  pattern  of  future  uses  of  the  value  of  an
expression.   The analysis  has two  parts.  First,  each  function is
summarized in a context-sensitive manner as a {\em demand transformer}
that transforms a {\em symbolic} demand  on its body to demands on its
arguments.  This summary  is then used to step  through function calls
during analysis.   Second, the concrete  demand on a function  body is
obtained through a 0-CFA-like conservative approximation that combines
the  demands on  all the  calls to  the function.   The result  of the
analysis  is the annotation  of each  program point  with finite-state
automata capturing  the liveness of variables at  the point. Depending
on the program point where  garbage collection has been triggered, the
GC consults  a set of automata  to curtail reachability
during marking.  This results in  an increase in the garbage reclaimed
and consequently in fewer garbage collections.

While this idea has been shown to be effective~\cite{asati14lgc} for a
first-order {\em  eager} language, a straightforward  extension of the
earlier  technique  is  not  possible  for  lazy  languages,  where  a
heap-allocated  object may either  be an  unevaluated expression  or a
closure. Since  data is  made live by  evaluation of closures,  and in
lazy languages  the place in  the program where this  evaluation takes
place cannot  be statically determined,  laziness complicates liveness
analysis itself.  In  addition, apart from evaluated data,  we want to
extend  liveness-based collection also  to closures---closures with no
future use should also  be collected.  This involves tracking closures
to  their points of  creation at  runtime from  where we  obtain their
liveness.  This  requires significant  modification to the  earlier GC
scheme.

{\color  {Myblue}Rewrite after the experiments:  Experiments  with a
single
  generation  copying  collector   confirm  the  expected  performance
  benefits.   Liveness-based  collection  results  in an  increase  in
  garbage reclaimed.   As a  consequence, there is  a decrease  in the
  number of  collections and a  (some quantification) decrease  in the
  memory size required  to run programs. Besides there  is a reduction
  in the overall execution time for some programs.}


%------------------------------------------------------------%
\subsection{Motivating Example}
\label{sec:motiv}

Figure~\ref{fig:mot-example} shows an example program and the state of
the heap at the program point $\pi$, i.e. just before the  evaluation
of $(\length\ \pz)$.  The heap is represented by a graph in which
a node  either represents atomic values ($\NIL$,  integers etc.), or
a \CONS\  cell  containing $\CAR$  and  $\CDR$  fields, or a chunk of
memory representing an unevaluated
value  also called a  {\em closure}  (represented by  hatched boxes).
Edges in the graph are  {\em references} and emanate from variables or
fields.  The figure shows the lists \px\ and \pz\ partially evaluated.
The evaluation was triggered by the test (\NULLQ~(\CAR~(\CDR~\pz))) in
the \SIF\ expression.


The edges shown by thick arrows  are those which are live at $\pi$.  A
cell is marked  and preserved during garbage collection  only if it is
reachable from the  root set through a path of  live edges.  All other
cells can be  reclaimed.  Thus if a garbage  collection takes place at
$\pi$  with  the  heap  shown  in  Figure~\ref{fig:mot-example}(b),  a
liveness-based collector (LGC) will  collect only the cells referenced
by  $\pz$, $(\CDR~  \pz)$  and (the  cells  constituting) the  closure
referenced by $(\CDR~(\CDR~\pz))$ need to be marked and preserved.  In
contrast,  a reachability-based collector  (RGC) would  have preserved
all cells.

{\color {Myblue}Rewrite at the end: The  specific contributions of the
paper
  are: The formulation of a  liveness analysis for a lazy language and
  the proof of its correctness, the undecidability of the grammar that
  results from the analysis and  its approximation by DFAs, the design
  of a  garbage collector that can  use the DFAs to  retain live cells
  that  also include  closures, and  empirical results  that  show the
  effectiveness of liveness-based garbage collection.}

\subsection{Organization of the paper}

Section~\ref{sec:defs} introduces the syntax and semantics of the
language used to illustrate our
analysis.
% Section~\ref{sec:operational} describes the operational semantics of
% the language.
Liveness analysis for this language is  described in
Section~\ref{sec:liveness}. This section also has by  a  sketch  of  a
correctness proof  relative  to  a  non-standard
semantics.  Section~\ref{sec:computing}  shows how to  encode liveness
as   finite-state  automata.    Section~\ref{sec:experiments}  reports
experimental  results  and Section~\ref{sec:lgc-always-better}  proves
that a liveness based collector  can never do more garbage collections
than a reachability based collector.
% and Section~\ref{sec:conclusion} concludes the paper.

\section{The target language---syntax and semantics}
\label{sec:defs}
%%We let $x$, $y$, $z$ range over variables, $f$ over user-functions
and
%% $p$    over    primitive    functions   ($\CONS$,    $\PRIM$ etc.).
Figure~\ref{fig:lang-syntax} describes the  syntax of our language. It
is a first order language with lazy semantics. Programs are restricted
to        be        in        Administrative        Normal        Form
(ANF)~\cite{chakravarty03perspective}  where all actual  parameters to
functions  are  variables.  While  this  restriction  does not  affect
expressibility,  this form  has  the benefit  of  making explicit  the
creation  of closures through  the $\LET$  construct.  $\LET$s  in our
language  are lazy;  in the  expression $\LET\,\,  x  \leftarrow s\,\,
\IN\,\, e$, $x$ may occur  in $s$. This enables creation of graph-like
structures in  a pure language. The  restriction of \LET\  to a single
definition  is  for ease  of  exposition---generalization to  multiple
definitions  does  not add conceptual  difficulties.  We  further
restrict each variable  in a program to be distinct,  so that no scope
shadowing   occurs---this  simplifies   proofs   of  soundness.    The
$\SRETURN$ and \SIF\ expressions trigger evaluation of closures.

The body  of a function ${\mathit  f}$ is denoted  as $e_{\mathit f}$.
We   shall   extend   this   convention   to   the   main   expression
$e_\mainpgm$. We assume that each program has a distinguished function
\mainpgm\      with      the      definition     $(\DEFINE\      ({\tt
  \mainpgm})\  e_\mainpgm)$ and  the execution  of the  program starts
with the call to \mainpgm.  We write $\pi\!:\!e$ to associate the label
$\pi$ (not  part of the language  syntax) with the  program point just
before expression $e$.

% \paragraph{Inlining.}
{ \color {Myblue} Relook after proof: In spite of the  ANF restrictions
it
  is still possible  to inline non-recursive functions (a  fact we use
  to  prove the safety  of liveness  analysis).  A  user-function call
  $(\LET\,\, x \leftarrow (f\,y_1\,\ldots\,y_n)  \,\, \IN\,\, e)$ to a
  function  defined  (after renaming  its  formals  and  locals to  be
  disjoint         from         existing         variables)         by
  $(\DEFINE\ (f\ z_1\ \ldots\ z_n)\ e_f)$ is replaced by a sequence of
  $\LET$'s of  the form  $z_i\leftarrow (\ID\,\,y_i)$ followed  by the
  body $e_f$  but with  its $(\SRETURN\,\,w)$ expressions  replaced by
  $(\LET\,\, x \leftarrow (\ID\,\, w) \,\, \IN\,\, e)$.}

\begin{figure}[t]
\footnotesize
\begin{eqnarray*}
   p \in \mathit{Prog} & ::= & d_1 \ldots d_n \,\, e_\mainpgm
    \hspace{5em} \mbox{\em --- program}\\
    \mathit{df} \in Fdef & ::= & (\DEFINE\,\, (f\,\, x_1 \,\, \ldots
\,\,x_n)\,\,
    e)
    \hspace{0.2em} \ \ \ \ \mbox{\em --- function definition} \\
e \in \mathit{Expr} & ::= &
\left\{\begin{array}{@{}ll@{\hspace{2em}}l}
       (\SIF\,\, x\,\, e_1\,\, e_2) && \!\!\!\mbox{\em --- conditional} \\
       (\LET\,\, x \leftarrow s\,\, \IN\,\, e) &&\!\!\! \mbox{\em --- let
binding} \\
       (\SRETURN\,\, x) && \!\!\!\mbox{\em --- return from function}
    \end{array}\right. \\
s \in \mathit{Application} & ::= &
\left\{\begin{array}{@{}l@{\hspace{1.2em}}l}
       k & \mbox{\em --- constant (numeric or $\NIL$)}\\
       (\CONS\,\, x_1\,\, x_2) & \mbox{\em --- constructor} \\
       (\CAR\,\, x) & \\
       (\CDR\,\, x) & \mbox{\em --- selectors} \\
       (\NULLQ\,\, x) & \\
       (\PRIM\,\, x_1\,\, x_2) & \mbox{\em ---  tester
and generic arithmetic} \\
       (\ID\,\, x) & \mbox{\em ---  identity function (for inlining)} \\
\\
       (f\,\, x_1\,\,\ldots\,\, x_n) & \mbox{\em --- function application}
    \end{array}\right.
\end{eqnarray*}
  \caption{The syntax of our language}\label{fig:lang-syntax}
\figrule
\normalsize
\end{figure}


\subsection{Semantics}
We now give  a small-step semantics for our  language.
We first specify the domains used by the semantics:
\[
\begin{array}{rlcl@{\hspace{0.5em}}l}
\rho: & \mathit{Env} &=&\mathit{Var} \rightarrow \mathit{Loc} &
\mbox{-- Environment} \\
v:   & \mathit{Val} &=& \mathbb{N} + \{\NIL\} + \mathit{Data \times
  Data}& \mbox{-- Values}\\
c:   & \mathit{Clo} &=& \mathit{(Exp \times Env)}& \mbox{--
  Closures}\\
d: & \mathit{Data} &=&\mathit{Val} + \mathit{Clo} & \mbox{-- Values \&
  Closures} \\
\heap: & \mathit{Heap} & =&\mathit{Loc} \rightarrow Data & \mbox{--
Heap}
\end{array}
\]

Here  $\mathit{Loc}$ is  a countable  set  of locations  in the  heap.
Since all data objects are boxed, we model an environment as a mapping
from the set  of variables of the program  $\mathit{Var}$ to locations
in the  heap.  These  either contain  a value in  WHNF (a  number, the
empty  list  $\NIL$,  or  a  \CONS\  cell  with  possibly  unevaluated
constituents) or  a {\em  closure}.  A closure  is a pair  $\langle s,
\rho\rangle$  in which  $s$  is an  unevaluated  application, and  the
values of its free variables is given by $\rho$.

The  semantics  of   expressions  (and  applications\footnote{In  most
  contexts, we  shall use  the term 'expression' and the notation $e$  for
both  expressions and
  applications.}) are  given by transitions  of the form  $\rho, \stk,
\heap, e \rightarrow \rho', \stk', \heap', e'$.  Here \stk\ is a stack
of  continuation  frames.  Each  continuation  frame  is  of the  form
$(\rho, x, e)$, signifying that the  location of $x$ has to be updated
with the  value of the currently  evaluating expression, and $e$  is to be
evaluated  next  in  the  environment  $\rho$.   The  start  state  is
$([\;]_\rho,([\;]_\rho,  \ans,   (\print~\ans)):[\;]_{S}  ,  [\;]_{H},
(\mainpgm)$.  In  this, the initial  environment and the  initial heap
are  denoted  by  $[\;]_\rho$  and  $[\;]_H$, and  the  initial  stack
consists  of  a  single  continuation   frame  in  which  \ans\  is  a
distinguished  variable  that  would  be  updated with  the  value  of
(\mainpgm).  In  addition, \print\ is a function  modelling a printing
mechanism---a standard  runtime support assumption  for lazy languages
that prints the  value of (\mainpgm).  Notice the  use of the operator
$:$ for adding elements to the top of the stack.




%% In absence of a static type-checking mechanism, programs with other
%% than syntactic  errors lead to states  not covered by  any rules in
%% the small-step semantics.
The notation  $[\myvec{x}   \mapsto  \myvec{\ell}]$   represents  an
environment that maps variables $x_i$
%$x_1,  \ldots, x_n$
to locations $\ell_i$
%$\ell_1,  \ldots,  \ell_n$,
and  $\heap[\ell :=  d]$
indicates the  updation of  a heap \heap\  at $\ell$ with  $d$.  $\rho
\oplus \rho'$  represents the  environment $\rho$ shadowed  by $\rho'$
and $\lfloor \rho \rfloor_X$  represents the environment restricted to
the locations in $X$. Finally $FV(s)$ represents the free variables in
the application $s$.

The small-step semantics  is shown in Figure~\ref{fig:lang-semantics}.
As  a sample,  consider  the  rules  for  $(+~x~y)$, an  application
involving a binary operator strict in both arguments.   When
both $x$  and $y$ are already  in WHNF ({\sc prim-whnf}),  the heap is
updated with the  sum of the arguments and  the next expression to be
evaluated is picked from the continuation on the stack. If either of the
arguments is not in
WHNF  ({\sc  prim-1-clo}  and   {\sc  prim-2-clo}),  it  is  sent  for
evaluation, and a continuation  marks that the evaluation of $(+~x~y)$
has to be resumed after the argument is evaluated.


\begin{figure*}[t!]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Premise & Transition & Rule name \\
\hline
\hline
          & $\rho, (\rho', x, e)\!:\!S, H, \kappa
  \rightsquigarrow \rho', S, H[\rho'(x) := \kappa], e$    &  \sc{const}
\\
\hline
          & {$\rho, (\rho', z, e)\!:\!S, H, (\CONS~x~y)
\rightsquigarrow
$  $\rho', S, H[z := (H(\rho(x)),H(\rho(y)))], e$}     &  \sc{cons} \\
\hline
$H(\rho(x)) \mbox{ is } (d_1, d_2)$ & $\rho, (\rho', z, e)\!:\!S, H,
(\CAR~x)  \rightsquigarrow \rho', S, H[\rho(z) := d_1], e$      &
\sc{car-whnf} \\
\hline
$H(\rho(x)) \mbox{ is } \langle s, \rho'\rangle$ & $\rho, S, H, (\CAR~x)
\rightsquigarrow
{\color {red}\rho\oplus\rho'}, (\rho, x, (\CAR~x))\!:\!S, H, s$      &
\sc{car-clo}
\\
\hline
$H(\rho(x)), H(\rho(y)) \in \mathbb{N}$
 & {$\rho, (\rho', z, e)\!:\!S, H, (+~x~y)  \rightsquigarrow$
$\rho', S, H[\rho(z) \mapsto H(\rho(x)) + H(\rho(y))], e$}      &
\sc{prim-whnf} \\
\hline
$H(\rho(x)) \mbox{ is } \langle s, \rho'\rangle$ & $\rho, S, H, (+~x~y)
\rightsquigarrow
{\color {red}\rho\oplus\rho'}, (\rho, x, (+~x~y))\!:\!S, H, s$      &
\sc{prim-1-clo} \\
\hline
$H(\rho(y)) \mbox{ is } \langle s, \rho'\rangle $ & $\rho, S, H, (+~x~y)
\rightsquigarrow
{\color {red}\rho\oplus\rho'}, (\rho, x, (+~x~y))\!:\!S, H, s$      &
\sc{prim-2-clo} \\
\hline
{$\mathit{f}~\mbox{defined as}$
$~(\DEFINE~(f~\myvec{y})~e_{\mathit{f}})$}  & $\rho, S, H,
(f~\myvec{x})  \rightsquigarrow
[\myvec{y} \mapsto \rho(\myvec{x})], S, H, e_{\mathit{f}}$      &
\sc{funcall} \\
\hline
$\ell$ is a new location& {$\rho, S, H, (\LET~x\leftarrow s~\IN~e)
  \rightsquigarrow$
$\rho\oplus[x \mapsto \ell], S, H[\ell \mapsto \langle s,
    \lfloor\rho\rfloor_{FV(s){\color {red}\setminus\{x\}}}  \oplus [x \mapsto
  \ell]\rangle], e$} &
\sc{let} \\
\hline
$H(\rho(x)) \ne 0$ & $\rho, S, H, (\SIF~x~e_1~e_2)   \rightsquigarrow
\rho, S, H,  e_1$ & \sc{if-true} \\
\hline
$H(\rho(x)) = 0$ & $\rho, S, H, (\SIF~x~e_1~e_2)   \rightsquigarrow
\rho, S, H,  e_2$ & \sc{if-false} \\
\hline
$H(\rho(x)) = \langle s, \rho' \rangle $ & {$\rho, S, H,
  (\SIF~x~e_1~e_2)   \rightsquigarrow
{\color {red} \rho \oplus \rho'}, (\rho, x, (\SIF~x~e_1~e_2))\!:\!S, H,  s$}
&
\sc{if-clo} \\
\hline
{$H(\rho(x))~\mbox{is}$ $\mbox{whnf with value}~v$}& $\rho, (\rho', z,
e)\!:\!S, H,
(\SRETURN~x)  \rightsquigarrow \rho', S, H[\rho(z) \mapsto v], e$ &
\sc{return-whnf}\\
\hline
$H(\rho(x)) = \langle s, \rho' \rangle $ & {$\rho, S, H, (\SRETURN~x)
  \rightsquigarrow$
${\color {red}\rho\oplus\rho'},~ (\rho, x, (\SRETURN~x))\!:\!S, H,  s$} &
\sc{return-clo} \\
\hline
\end{tabular}
\caption{A small-step semantics for the language. {\color {red} Can we
    replace  $\rho   \oplus  \rho'$  by  $\rho'$?  Can   we  drop  the
    $\setminus\{x\}$         in          the         {\sc         let}
    rule?}\label{fig:lang-semantics}}
\end{center}
\end{figure*}


%==============================================================
\renewcommand{\pp}[2]{\ensuremath{#1\!\!:\!#2}} % prog point



\section{Liveness}\label{sec:liveness}

A variable is {\em live} if  there is a possibility of its value being
used in  future computations  and dead if  it is definitely  not used.
Heap-allocated data needs a richer model of liveness which talks about
liveness  of references.   Using  $\acar$, $\acdr$  to represent  access
using  $\CAR$  and  $\CDR$  fields,  the  liveness  of  the  structure
reachable from a  variable can be represented by a  set of {\em access
  paths} i.e.  prefix-closed  strings from $\{\acar,\acdr\}^\ast$.  As
an  example,  if $x$  is  a  list  with liveness  $\{\epsilon,  \acdr,
\acdr\acar,  \acdr\acdr, \acdr\acdr\acar\}$, then  future computations
can only  refer up  to the second  and third  members of $x$.   A {\em
  liveness  environment} is  a mapping  from variables  to  subsets of
$\{\acar,\acdr\}^\ast$, but  often expressed as a set,  for example by
writing $\{x.\epsilon, x.\acdr, x.\acdr\acdr, y.\epsilon\}$ instead of
$[x     \mapsto\{\epsilon,\acdr,\acdr\acdr\},    y\mapsto\{\epsilon\},
  z\mapsto\{\}]$.    In  this   notation,  $x   \mapsto  \{\epsilon\}$
represents access using $x$ itself and $x \mapsto \{\}$ indicates that
$x$ is dead.  We  associate liveness environments with program points.
The liveness at $\pi:e$ is the liveness just before executing $e$.


A  notion  related  to  liveness  is  {\em  demand}.   While  liveness
represents the future use of variables, a demand represents the future
use of the value of an  expression. The demand on an expression $e$ is
again a  set of  access paths---that subset  of $\{\acar,\acdr\}^\ast$
which the context of $e$ may explore of $e$'s result.  To see the need
for  demands,  consider   the  let  expression  $\pi:(\LET~x\leftarrow
(\CDR~y)~\IN~  \pi':\SRETURN~x)$.   Assume that  the  context of  this
expression produces  the liveness $[x \mapsto  \lbrace \epsilon, \acar
  \rbrace]$ at $\pi'$.  Due to the \LET\ definition which binds $x$ to
$(\CDR~y)$, the  liveness of $x$ at  $\pi'$ now becomes  the demand on
$(\CDR~y)$.    This,  in   turn,  generates   the   liveness  $\lbrace
y.\epsilon, y.\acdr, y.\acdr\acar \rbrace$ at $\pi$. These are the set
of $y$-rooted  accesses required  to explore $\lbrace  \epsilon, \acar
\rbrace$  paths   of  the  result   of  $(\CDR~y)$\footnote{Reproduced
  from~\cite{asati14lgc}: In  classical strong liveness  analysis, $y$
  and $z$  are live at the  entry $n: x:=y+z$,  if and only if  $x$ is
  live  at exit  of $n$.   In our  terminology, the  liveness $\lbrace
  \epsilon\rbrace $ of $x$ at the  exit from $n$ becomes the demand on
  $y+z$, and this, in turn generates the liveness $\lbrace y.\epsilon,
  z.\epsilon \rbrace$ at the entry of $n$.}.
%% Exploring $\lbrace \epsilon, \acar \rbrace$ of the
%% result of $(\CDR~y)$ first requires access using $y$ ( and the \CDR\
%% field of the cons cell pointed by
%% $y$.


Note that  liveness refers to  access using variables and  fields, and
not  to $\CONS$  cells (i.e.\  to edges  in the  memory graph,  not to
locations themselves).
We use $\sigma$  to range over demands, $\alpha$  to range over access
paths  and $\Lv$ to  range over  liveness environments.   The notation
$\sigma_1\sigma_2$  denotes  the  set $\lbrace  \alpha_1\alpha_2  \mid
\alpha_1 \in \sigma_1, \alpha_2  \in \sigma_2\rbrace$.  Often we shall
abuse notation to  juxtapose an edge label and a  set of access paths:
$\acar\sigma$ is a shorthand for $\lbrace\acar\rbrace\sigma$. A more
elaborate explanation of these notions appear in \cite{asati14lgc}


\subsection{Liveness Analysis for lazy languages}

Figure~\ref{fig:live-judge}  describes  our  analysis  which  has  two
parts. The function $\mathit{ref}$,  takes  an application $s$ and
a demand $\sigma$, and returns  the incremental liveness generated for the
free  variables   of  $s$  due  to  the   application.   The  function
$\mathcal{L}$  uses   $\mathit{ref}$  to  propagate   liveness  across
expressions.

 Since  in a  lazy language,  an  expression is  not evaluated  unless
 required, the null demand ($\emptyset$) does not generate liveness in
 any  of  the  rules  defining  $\mathit{ref}$  or  $\mathcal{L}$.   A
 non-null  demand of  $\sigma$ on  (\CDR~$x$), is  transformed  to the
 liveness $\{x.\epsilon, x.\acdr\sigma\}$.   In an opposite sense, the
 demand  of $\acdr\sigma$  on  (\CONS~$y$~$z$) is  transformed to  the
 demand  $\sigma$ on  $z$.   Note  that \CONS\  does  not, by  itself,
 dereference its  arguments. The rules for  (\PRIM~x~y) and (\NULLQ~x)
 are similar. Constants do not generate any liveness.

For  the  case  of a  user-function  call  we  use a  third  parameter
$\Lfonly$  that  represents the  summaries  of  all  functions in  the
program.   $\Lfonly_{\mathit f}$  (the  component of  $\Lfonly$ for  a
specific function $f$), expresses how the demand $\sigma$ on a call to
$f$  is  transformed  into  the  liveness of  its  parameters  at  the
beginning of the call.  $\Lfonly$  is determined by the judgement form
$\mathit{Prog} \len \Lfonly$ using inference rule ({\sc live-define}).
This  rule  describes the  fixed-point  property  to  be satisfied  by
$\Lfonly$, namely, the demand transformation assumed for each function
in  the  program should  be  the  same  as the  demand  transformation
calculated    from    its    body.      As    we    shall    see    in
Section~\ref{sec:grammar-formulation},  we  convert  the rule  into  a
grammar  and the  the  language  generated by  this  grammar is  least
solution satisfying  the rule. We  prefer the least solution  since it
ensures the safe collection of the greatest amount of garbage.

We next  describe the function $\mathcal{L}$  that propagates liveness
across expressions.  Note that,  unlike eager languages, the syntactic
occurrence  of an expression  in a  program denotes  the point  of
creation of its closure, not necessarily its evaluation.
To see the  consequences of this, consider
Figure~\ref{fig:mot-example2} which shows an analysis  of the body of
a function $\length$ with a demand $\sigma$.  Clearly, this is also the
demand on the return value $z$ giving the liveness at the program point
$\pi_8$ as $\{\pz.\sigma\}$.  The liveness at $\pi_8$ is propagated
backwards as in traditional liveness analysis.

Since   \pz\  is   $(1  +   \py)$,  the   liveness  of   $\sigma$  for
\pz\ translates into a demand  of $\sigma$ on $(1+\py)$, which in turn
generates a  liveness of $\{\py.\epsilon\}$ at  $\pi_7$.  We now make
the following observations:
\begin{enumerate}
\item While the liveness of \py\  should be killed at $\pi_6$ since it
  is  beyond the scope  of the  definition of  \py, we  do not  do so.
  However, this  does not cause any  imprecision in the way  of the GC
  marking more cells as live,  since, during execution, \py\ is yet to
  come  into  existence at  $\pi_6$,  and  its  liveness will  not  be
  consulted by the garbage collector at this point.
\item In an eager language, \py\ would not have been live at $\pi_8$,
  a program point  beyond its last use.  In  a lazy language, however,
  the  definition $\pz  \leftarrow  (1+\py)$, does  not  result in  an
  evaluation  of  $(1+\py)$;  instead   a  closure  is  created.   The
  evaluation  actually takes  place while  evaluating $(\SRETURN~\pz)$.
  We therefore  record  the liveness of $\py$ at the program
  point $\pi_8$,  indeed at every  program point from $\pi_8$  up to
  the beginning  of the program $\pi_1$ (since  we do not  kill
liveness).
\end{enumerate}



We thus regard  the body of the function as a  tree and consider paths
from  the  root   of  this  tree  to  program   points  which  trigger
evaluation\footnote{Program   points  which  trigger   evaluation  are
  labelled  with   $\psi$  }---\SIF\ conditions  and \SRETURN\
expressions.   We call such
paths {\em e-paths}.   For the function \length, there  are three such
paths  $ep_1   =  \langle   \pi_1,  \pi_2,  \psi_1\rangle$,   $ep_2  =
\langle\pi_1, \pi_2, \pi_3, \pi_4,  \psi_2\rangle$ and $ep_3 = \langle
\pi_1, \pi_2, \pi_5, \pi_6, \pi_7, \pi_8, \psi_3\rangle$.

%==============================================================
\begin{figure*}[t]
\begin{eqnarray*}
\mathit{ref\/}(\kappa,\sigma,\Lfonly)
          &=& \{\,\} \mbox{, for $\kappa$ a constant, including
$\NIL$}\\
\mathit{ref\/}((\CONS~x~y),\sigma,\Lfonly)
          &=& \{x.\alpha \mid \acar\alpha \in \sigma\} \cup \{y.\alpha
\mid \acdr\alpha \in \sigma\} \\
\mathit{ref\/}((\CAR~x),\sigma,\Lfonly)
          &=&    \begin{array}{l l}
                    \{x.\epsilon\} \cup \{x.\acar\alpha \mid \alpha \in
\sigma\}, & \mbox{if}~\sigma \ne \emptyset\\
                    \emptyset  & \mbox{otherwise}
                 \end{array} \\
\mathit{ref\/}((\CDR~x),\sigma,\Lfonly)
          &=&    \begin{array}{l l}
                    \{x.\epsilon\} \cup \{x.\acdr\alpha \mid \alpha \in
\sigma\}, & \mbox{if}~\sigma \ne \emptyset\\
                    \emptyset  & \mbox{otherwise}
                 \end{array} \\
\mathit{ref\/}((\PRIM~x~y),\sigma,\Lfonly)
          &=&    \begin{array}{l l}
                    \{x.\epsilon, y.\epsilon\},  & \mbox{if}~\sigma \ne
\emptyset\\
                    \emptyset  & \mbox{otherwise}
                 \end{array} \\
\mathit{ref\/}((\NULLQ~x),\sigma,\Lfonly)
          &=&    \begin{array}{l l}
                    \{x.\epsilon\},  & \mbox{if}~\sigma \ne \emptyset\\
                    \emptyset  & \mbox{otherwise}
                 \end{array} \\
\mathit{ref\/}((f~\myvec{x}),\sigma,\Lfonly)
%          &=& \bigcup_{i=1}^n y_i.\Lf{f}{i}{\sigma}
          &=&  \begin{array}{@{}l}  % to discourage \displaystyle
               \bigcup_{i=1}^n x_i.\Lf{f}{i}{\sigma}
               \end{array}
%          &=& \bigcup \{y_i.\Lf{f}{i}{\sigma} \mid i=1,\ldots, n\}
\\[1ex]
\mathcal{L}((\SRETURN~\psi:x),\sigma,\Lfonly) &=& \lbrack \mathit{ep}
\mapsto
x.\sigma\rbrack, \mbox{ where $\mathit{ep}$ is a
  new e-path terminating with $\psi$}\\
\mathcal{L}((\SIF~x~e_1~e_2),\sigma,\Lfonly) &=&
        \begin{array}{l l}
                    \mathcal{L}(e_1,\sigma,\Lfonly) \uplus
        \mathcal{L}(e_2,\sigma,\Lfonly) \uplus
        \lbrack\lbrace \mathit{ep} \mapsto
\{x.\epsilon\}\rbrace\rbrack,  & \mbox{if}~\sigma \ne \emptyset\\
        \emptyset_{\cal{M}?}  & \mbox{otherwise}
                 \end{array} \\
&&\mbox{where $\mathit{ep}$ is a new e-path
  terminating with $\psi$}\\
\mathcal{L}(\LET~x \leftarrow~s~\IN~e),\sigma,\Lfonly) &=&
        \lambda\;\mathit{ep}.\; \Lfonly_f(\Lv(x)) \cup \Lv~~
\mbox{where}~\mathcal{M} =
\mathcal{L}(e,\sigma,\Lfonly),~\Lv \in
\mathcal{M}(ep),~\mbox{and}~f~\mbox{is the new function}\\
&& \makebox[0mm]{\hspace*{7cm}
 $(\DEFINE~(f\,x_1\,x_2\, \ldots \, x_n)~(\SIF\,*~x~s[(f\,x_1\,
           x_2\, \ldots\, x_n)/x]))$,} \\
 \makebox[0mm]{\hspace*{8cm} where
     $x_1\, \ldots\, x_n$ are the variables in $s$}
\end{eqnarray*}
\begin{minipage}{0.85\textwidth}
\infrule[live-define]
        {\mathcal{L}(e_f,\sigma,\Lfonly) =
           \bigcup_{i=1}^n z_i.\Lf{f}{i}{\sigma}
              \mbox{ for each $f$ and $\sigma$}
        }
        { \mathit{df_1} \ldots \mathit{df_k} \len \Lfonly
\\ \makebox[0mm]{where
     $(\DEFINE\ (f\ z_1\ \ldots\ z_n)\ \ e_f)$ is a member of $\mathit{df_1}
\ldots \mathit{df_k}$}}
\end{minipage}
  \caption{Liveness equations and judgement rule}\label{fig:live-judge}
\end{figure*}



Given  a function  $\mathit{f}$ and  a demand  $\sigma$,  the liveness
analysis  computes  two   maps:  $\mathcal{L}(e_f,  \sigma,  \Lfonly)$
computes a  map that takes a  e-path in ${\mathit f}$  and returns the
liveness environment  arising out of  all uses of variables  along the
e-path.  A second map $\cal{P}_{\mathit  f}$ maps any program point in
$\mathit f$  to the set  of e-paths sharing  the point.  If  a program
point happens to fall on more  than one such e-path, then the liveness
environment of  the program  point is the  variable-wise union  of the
liveness  environments  of the  individual  e-paths.   In the  example
program,  since ${\mathcal P}_{\length}(\pi_1)  = \lbrace  ep_1, ep_2,
ep_3\rbrace$, the  liveness at $\pi_1$  is ${\cal M}(ep_1)  \cup {\cal
  M}(ep_2) \cup  {\cal M}(ep_3)$,  where ${\cal M}  = \mathcal{L}(e_f,
\sigma, \Lfonly)$.  In the  sequel we shall only define $\mathcal{L}$.
The formulation  of $\cal{P}$ is easy  and we do not  elaborate it any
further.

Consider  the $\mathcal{L}$-rules for  {\LET}, {\SIF},  and {\SRETURN}.
The  expression $(\SRETURN~\psi:x)$  with a  demand $\sigma$  forces an
evaluation  of   $x$  and  hence  a  new   map  $[\mathit{ep}  \mapsto
  \{x.\sigma\}] $  needs to  be created, where the $ep$ is a fresh
evaluation path terminating in $\psi$.  The  map for  the expression
$(\SIF~\psi:x~e_1~e_2)$ is a pointwise-union  of the maps of $e_1$ and
$e_2$. In addition, since the condition also triggers an evaluation of
$x$,  the map  $[\mathit{ep} \mapsto  \{x.\epsilon\}]$ is  created and
added  to the  union.  Also  notice a  consequences  of laziness---the
entire  expression including  the condition  is not  evaluated  if the
demand on it is $\emptyset$.  This results in the empty map, which, by
abuse of notation, is also denoted by $\emptyset_{\cal{M}}$.



 The liveness  of {\LET} has  an elegant formulation.   Recollect that
 our language permits lazy {\LET}s whose definitions can be recursive,
 i.e.  in the expression $\LET~x \leftarrow~s~\IN~e$, $x$ can occur in
 $s$.   We  model  the  definition  $x  \leftarrow~s$  as  a  possibly
 recursive function (named $\mathit{f}$ in the rule) which captures an
 arbitrary number of ``unrollings'' of the definition\footnote{The $*$
   in  the definition  of ${\mathit  f}$  represents non-deterministic
   choice.  During   liveness  it  can  be  assumed   to  generate  no
   liveness.}.   We pass to  the demand  transformer $\Lfonly_{\mathit
   f}$ of this function the demand  arising out of the liveness of $x$
 at the beginning of $e$. This  gives the liveness of the variables of
 $s$.


\mycomment{The function  $\mathcal{L}$ now gives  the (total) liveness
  of  an   expression  $e$.   The  cases  $\SRETURN$   and  $\SIF$  are
  straightforward, but note the liveness $x.\epsilon$ generated by the
  latter.   The case  $(\LET\  z\leftarrow s\  \IN\  e')$ resembles  a
  three-address instruction:  the liveness of  $e$ is given  by taking
  the liveness, $\Lv$, of $e'$, killing any liveness of $z$ and adding
  any incremental  liveness from  $s$.  The main  subtlety is  how the
  liveness of  $z$ in $\Lv$  is converted to  a demand $\Lv(z)$  to be
  placed on $s$ via $\mathit{ref}(s,\Lv(z),\Lfonly)$.


We make three observations: firstly the rule ({\sc live-define}) has a
least  solution  as  $\mathcal{L}(\cdot)$  is monotonic  in  $\sigma$;
secondly  that  ({\sc  live-define})   resembles  the  rule  for  type
inference of mutually recursive  function definitions, and thirdly the
asymmetry of  demand and liveness (compared to  post- and pre-liveness
classically) is due to the functional formulation here.
}%mycomment

Section~\ref{sec:computing}   shows   how   the  demand   transformers
$\Lfonly$  for  a  program  (representing  a  fully  context-sensitive
analysis) can be safely approximated by a {\em procedure summary}. for
each function.   The summary  is in the  form of a  demand transformer
that maps a demand on a call to the function to demands on each of its
arguments.

{\color{red} Proof of correctness of the liveness analysis...}


\section{Computing liveness and its encoding as DFA}\label{sec:computing}
Section~\ref{sec:liveness} gave  a context-sensitive liveness analysis
and  {\color  {red}  proved  it  correct  with  reference  to  a  {\em
    minefield} semantics}.  In  particular, $\mathcal{L}$ and \pmap{f}
together described a liveness set for each program point in a function
in terms of  a given $\sigma$ and \Lfonly.  However,  we still have to
describe how to obtain demand transformers \Lfonly\ from the rule {\sc
  live-define} and how to compute the specific demand $\sigma$ on each
function.

To  answer these  questions, it  is convenient  to cast  the equations
arising out of $\mathit{ref}$  and $\mathcal{L}$ as the rules of a
grammar.   But first  we modify  the rules  themselves to  a different
form.

\subsection{Modifying the liveness rules.}

The      $\mathit{ref}$     rule      for     \CONS,      shown     in
Figure~\ref{fig:live-judge},  requires   us  to  remove   the  leading
\acar\ and \acdr\  from the access paths in  $\sigma$.  Similarly, the
rules for  \CAR, \CDR, \PRIM, \NULLQ,  and \SIF\ require  us to return
$\emptyset$, if  $\sigma$ itself is $\emptyset$ and
$\lbrace\epsilon\rbrace$ otherwise.  To  realize these rules
$\sigma$ needs to be known. This creates difficulties since we want to
solve the equations arising out of liveness symbolically.

The  solution  is  to   also  treat  the  operations  mentioned  above
symbolically.   We  introduce three  new  symbols.  These symbols  are
defined as a relation  $\hookrightarrow$ between sets of access paths.
First, \bcar\ and \bcdr\ are defined as:

\begin{align*}
  &\sigma_1\bcar\sigma_2  \hookrightarrow
  \sigma_1\sigma_2'\mbox{ where } \sigma_2 = \acar\sigma_2',\\
&\sigma_1\bcdr\sigma_2  \hookrightarrow
  \sigma_1\sigma_2'\mbox{ where } \sigma_2 =
  \acdr\sigma_2'
\end{align*}
Thus \bcar, for example, removes the leading \acar\ from $\sigma_2$.
Similarly, the symbol \clazy\ reduces the string following it to an
$\epsilon$. \clazy\  is defined by addition to
$\hookrightarrow$ the rule:
\begin{align*}
  &\sigma_1\clazy\sigma_2  \hookrightarrow
  \sigma_1 \mbox{ if } \sigma_2 \subseteq  \{\acar,\acdr\}^\ast
\end{align*}
%% \begin{align*}
%%  \bcar\sigma &\triangleq \{\alpha \mid 0\alpha \in \sigma\}\\
%%  \bcdr\sigma &\triangleq \{\alpha \mid 1\alpha \in \sigma\}
%% \end{align*}
%% and \clazy\ as:
%% \begin{align*}
%%  \clazy\sigma \triangleq\; & \emptyset~\mbox{if}~\sigma = \emptyset\\
%%                        & \{\epsilon\} ~\mbox{otherwise}
%% \end{align*}
%% Note that \clazy\ is idempotent.
We can  now rewrite the \CONS\  and the \CAR\  rules of $\mathit{ref}$
as:
\begin{align*}
&\mathit{ref\/}((\CONS~x~y),\sigma,\Lfonly)
= x.\bcar\sigma \cup y.\bcdr\sigma  \label{eqn:mod-cons},~
\mbox{and} \\
&\mathit{ref\/}((\CAR~x),\sigma,\Lfonly)
          =   x.\clazy\sigma \cup x.0\sigma
\end{align*}
and the \Lfunonly\ rule
for \SIF\ as:
\begin{align*}
\mathcal{L}((\SIF~x~e_1~e_2),\sigma,\Lfonly) =
                    &\mathcal{L}(e_1,\sigma,\Lfonly)~\uplus
        \mathcal{L}(e_2,\sigma,\Lfonly)~\uplus\\
        &[\,\mathit{ep} \mapsto  x.\clazy \sigma]
\end{align*}
The rules for  \CDR, \PRIM\ and \NULLQ\ are also  modified in a manner
similar to \CAR.  %% The definitions  of \bcar\, \bcdr\ and \clazy\ induce
%% the following relation $\hookrightarrow$ over sets of access paths:
%% \begin{align*}
%%   &\sigma_1\bcar\sigma_2  \hookrightarrow
%%   \sigma_1\sigma_2'\mbox{ where } \sigma_2 = \acar\sigma_2',\\
%% &\sigma_1\bcdr\sigma_2  \hookrightarrow
%%   \sigma_1\sigma_2'\mbox{ where } \sigma_2 =
%%   \acdr\sigma_2',~\mbox{and}\\
%% &\sigma_1\clazy \sigma_2 \hookrightarrow \sigma_1, \mbox{ if }
%% \sigma_2 \ne
%%   \emptyset
%% \end{align*}
The reflexive  transitive closure of  $\hookrightarrow$ will
be   denoted    as   $\stackrel{*}{\hookrightarrow}$.    The
following proposition relates  the original and the modified
liveness rules.

\begin{figure*}[t!]
  \begin{tabular}{cc}
    \begin{minipage}{.40\textwidth}
        \small
        \renewcommand{\arraystretch}{1}{
          \begin{uprogram}
            \UNL{1} (\DEFINE\ (\length~\xl)
            \UNL{2}  $\pi_1\!\!:\, $(\LET\ \px\ $\leftarrow $\
(\NULLQ~\xl) \IN
            \UNL{3} \hspace*{.05cm} $\pi_2\!\!:\,$(\SIF\
$\psi_1\!\!:\,$ \px
            \UNL{4} \hspace*{.27cm} $\pi_3\!\!:\,
            $(\LET\ \pv\ $\leftarrow 0$ \IN
            \UNL{5} \hspace*{.32cm} $\pi_4\!\!:\,
(\SRETURN~\psi_2:\pv)$
            \UNL{4} \hspace*{.29cm}    $\pi_5\!\!:\, $(\LET~\pu\
$\leftarrow$  (\CDR~\xl)  \IN
            \UNL{5} \hspace*{.34cm}   $\pi_6\!\!:\, $(\LET~\py\
$\leftarrow$  (\length~\pu)  \IN
            \UNL{6} \hspace*{.34cm} $\pi_7\!\!:\,
            $(\LET~\pz\ $\leftarrow$ (1~+~\py) \IN
            \UNL{7} \hspace*{.34cm} $\pi_8\!\!:\,
(\SRETURN~\psi_3:\pz)$)))))))
        \end{uprogram}}
        \renewcommand{\arraystretch}{1}{
	  \begin{uprogram}
	  \UNL{1} $(\DEFINE\ (\main)$
           \UNL{2} \!\!$\pi_9\!\!:\, \cred{(\LET\  \pa\  \leftarrow
(\abigfunction\ \acdr)}$ \IN
          \UNL{3} \!\!$\pi_{10}\!\!:\, \cred{(\LET\  \pb\  \leftarrow
(}$\cred{+}$\cred{\ \pa\ \acdr)}$
\IN
	  \UNL{4}   \hspace*{.05cm}$\pi_{11}\!\!:\,      $ \cred{(\LET\ \pc}\
$\cred{\leftarrow  (\CONS\ \pb\ ())}$ \IN
          \UNL{5}   \hspace*{.15cm}    $\pi_{12}\!\!:\,
          $\cred{(\LET\ \pw}\  $\cred{\leftarrow  (\length\ \pc)}$ \IN
          \UNL{6}  \hspace*{.25cm}  $\pi_{13}\!\!:\,
(\SRETURN~\psi_4:\pw)))))$
\end{uprogram}}
        %%}
    \end{minipage}

    &

    \begin{minipage}{.51\textwidth}

        \small
\begin{eqnarray*}
        \mathcal{L}(e_{\length},\sigma,\Lfonly) &=&
              [\;ep_1 \mapsto \lbrace \px.\clazy \sigma,
\xl.\clazy\sigma\rbrace ,~ep_2 \mapsto \lbrace \cred{
\pv.\sigma}  \rbrace,\\
 &&          \;\;  ep_3 \mapsto \lbrace \pz.\sigma,
                \py.\clazy\sigma,
\pu.\Lf{\length}{\mbox{1}}{\clazy\sigma},\\
&&            \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\xl.\{ \acdr\Lf{\length}{\mbox{1}}{\clazy\sigma} \cup
%%\;\;\;\;\;\;\; \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\\
\cred{\clazy\Lf{\length}{1}{\clazy\sigma}}\}\}
             \rbrack \\
            \cal{P_{\mbox{\tiny \length}}} &=&
                [\;\pi_1 \mapsto \lbrace ep_1, ep_2, ep_3 \rbrace,  \
                 \pi_2 \mapsto \lbrace ep_1, ep_2, ep_3
                  \rbrace, \\
&&\;\;  \pi_3 \mapsto \lbrace ep_2
                  \rbrace ,  \pi_4 \mapsto \lbrace ep_2
                  \rbrace, \pi_5 \mapsto \lbrace  ep_3 \rbrace, \\
 &&  \;\;                        \pi_6 \mapsto \lbrace ep_3 \rbrace,
\pi_7 \mapsto
                   \lbrace ep_3\rbrace, \pi_8 \mapsto \lbrace ep_3
\rbrace~\rbrack\\
              \mathcal{L}(e_{\main},\sigma,\Lfonly) &=&
                [\;
                  ep_4 \mapsto \lbrace \cred {\pw.\sigma,
                                       \pc.\Lf{\length}{\mbox{1}}{\sigma},}\\
&& \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;
\cred{\pb.\bcar\Lf{\length}{\mbox{1}}{\sigma},}
\cred{\pa.\clazy\bcar\Lf{\length}{\mbox{1}}{\sigma}} \rbrace
                               ] \\
               \cal{P_{\mbox{\tiny \main}}} &=&
                  \lbrack~ \cred{\pi_9 \mapsto \lbrace ep_4 \rbrace,  \
                    \pi_{10} \mapsto \lbrace ep_4 \rbrace,  \
                   \pi_{11} \mapsto \lbrace ep_4 \rbrace,} \\
                   && \;\; \cred{\pi_{12} \mapsto \lbrace ep_4 \rbrace,
\pi_{13} \mapsto \lbrace ep_4 \rbrace}~\rbrack
\end{eqnarray*}
    \end{minipage} \\
(a)&
(b)
    %% \end{ tabular}}
  \end{tabular}
%\end{picture}
\kern -3ex
%\vspace{0.5in}
\caption{(a) Example program and (b) its liveness
maps.}\label{fig:mot-example2}
\end{figure*}
%================================================================


\begin{proposition}
Assume that a liveness computation  based on the original set of rules
gives the liveness  of the variable $x$ at a  program point $\pi_i$ as
$\sigma$   (symbolically,   $\Lanv{i}{x}=   \sigma$).   Further,   let
$\Lanv{i}{x}=  \sigma'$ when the  modified rules  are used  instead of
\Lfunonly.  Then $\sigma' \stackrel{*}{\hookrightarrow} \sigma$.
\end{proposition}

For  an explanation  of why  the  proposition holds  for the  modified
\CONS\  rule,   we  refer   the  reader  to   \cite{asati14lgc}.   The
proposition also holds for other modified rules for similar reasons.

%Thick red line
%{\color{red} \medskip \hrule \hrule \hrule  \medskip }

\newcommand{\emm}[2]{\ensuremath{\mathcal{#1}_{#2}}}

\subsection{Generating Liveness Equations}
The  program  in  Figure~\ref{fig:mot-example2}  serves as  a  running
example.   Given a  function $\mathit{f}$,  we now  describe  how to
generate    equations    for     the    demand    transformation
\Lfonly$_\mathit{f}$.   Starting with a  symbolic demand  $\sigma$, we
determine  \Lfun{e_{\mathit{f}}}{\sigma}{\Lfonly}.  In  particular, we
consider $\Lanv{1}{x_i}$,  the  liveness of    the
$i^{\mbox{\footnotesize  th}}$
parameter $x_i$ at the  program point at the  beginning of
$e_{\mathit{f}}$ (assumed  to be  $\pi_1$). By the rule {\sc
  live-define}, this should be the
same as $\Lf{f}{i}{\sigma}$. Applying this to  \length, we have:
\begin{eqnarray*}
&& \Lanv{1}{\xl} = \clazy\sigma \cup \acdr\Lf{\length}{1}{\clazy\sigma}
  \cup \clazy\Lf{\length}{1}{\clazy\sigma}
\end{eqnarray*}
and  the only equation defining \Lfone{\length} is:
  \begin{eqnarray*}
   && \Lf{\length}{1}{\sigma}
    =  \clazy\sigma \cup \acdr\Lf{\length}{1}{\clazy\sigma} \cup
\clazy\Lf{\length}{1}{\clazy\sigma}
\end{eqnarray*}

In general, the equations for  \Lfonly\ are recursive since, as in the
case  of  \length,  \Lan{1}\  may   have  been  defined  in  terms  of
\Lfone{\mathit{f}}.  However,  it is desirable  to have a  closed form
solution for $\Lfone{\mathit{f}}$.  As mentioned in \cite{asati14lgc},
each  of the liveness  rules modifies  a demand  by prefixing  it with
symbols  in the  alphabet  $\lbrace \acar,  \acdr,\bcar, \bcdr,  \clazy
\rbrace$, and therefore we can assume that $\Lf{f}{i}{\sigma}$ has the
closed form:
\begin{eqnarray}
\label{eq:LF:DI}
  \Lf{f}{i}{\sigma} = \Df{f}{i}\sigma
\end{eqnarray}
where \Df{f}{i} are sets of strings over the alphabet mentioned above.
Substituting   the   guessed   form   in   the   equation   describing
\Lfonly$_{\mathit f}$, and factoring  out $\sigma$, we get an equation
for  \Df{f}{i} that  is  independent of  $\sigma$.   Any solution  for
\Df{f}{i}  yields a  solution for  \Lfonly$_{\mathit f}$.   Applied to
\Lfonly$_{\mathit \length}$, we get:
  \begin{eqnarray*}
&&  \Lf{\length }{1}{\sigma} = \Df{\length}{1}\sigma,~\mbox{and}\\
&&   \Df{\length}{1} = \clazy \cup \acdr\Df{\length}{1}\clazy
       \cup \clazy\Df{\length}{1}{\clazy}
  \end{eqnarray*}

Note that this equation can also be viewed as a CFG with \{\acdr,
\clazy\} as terminal symbols and \Df{\length}{1} as the sole
non-terminal.

\subsection{Generating liveness equations \Lv\  for function bodies}
\label{sec:bodylivenessbodies}

To avoid analyzing the body of a function for each call, we calculated
the liveness  for the arguments and  the variables in  a function with
respect to a symbolic demand  $\sigma$.  To get the actual liveness we
calculate an over-approximation of the  actual demands made by all the
calls by a {\em summary demand}  and calculate the liveness at each GC
point inside the  function based on this summary  demand.  Even though
we  lose precision  due to  this, our  experiments show  that  this is
acceptable  \comment{Is  this  claim  true?}.   The  0-CFA-style  {\em
  summary demand}  is calculated by taking  a union of  the demands at
every call site of a function.

Consider a function $g$ containing a  call to $f$ at a site $\pi$, say
$\pi\!\!:\!(\LET~x   \leftarrow   (f\,y_1\,\ldots\,y_n)~\IN~\pi_i:e)$.
Let the  demand on $g$  be $\sigma_g$ and,  based on this  demand, the
liveness of  $x$ at $\pi_i$ be  $\Lanv{i}{x}$.  By the  $\LET$ rule of
Figure~\ref{fig:live-judge},    the   call   at    $\pi$   contributes
$\Lanv{i}{x}$ to the demand $\sigma_f$. Let us denote the contribution
of a call  site $\pi$ in a  function $g$ to the overall  demand on the
function $f$  as $\deltacall{f}{\pi}{g}$. Assuming that  there are $k$
call sites to function $f$, $\pi^1$ (in function $g^1$) \ldots $\pi^k$
(in   function  $g^k$),  the   over-approximation  of   $\sigma_f$  is
$\deltacall{f}{\pi^1}{g^1}          \cup          \cdots          \cup
\deltacall{f}{\pi^k}{g^k}$.  The distinguished function \mainpgm\ is a
special case.   We assume  it is called  through a  printing mechanism
with   demand   $\sigma_\mainpgm   =  \{\acar,\acdr\}^\ast$ (denoted
$\sigma_{\!all}$) if \mainpgm\ returns a structure and $\epsilon$ if
it returns a base value.


For  the  running  example,   $\length$  has  calls  from  $\main$  at
$\pi_{12}$ and a recursive call at $\pi_6$.
So $\sigma_{\length} =
     \deltacall{\length}{\pi_{12}}{\main}  \cup
\deltacall{\length}{\pi_6}{\length}$.
Filling in  the values gives:
\begin{eqnarray*}
\sigma_{\length}    &=&
 \{\epsilon\}  ~\cup~{\clazy\sigma_{\mathit{\length}}}
\end{eqnarray*}
As examples, the liveness of \Lanv{1}{\pl} and \Lanv{9}{\pa} in terms
of  $\sigma_{\length}$ are:
\begin{align*}
\Lanv{1}{\pl} &= (\clazy \cup \acdr\Df{\length}{1}\clazy
  \cup \clazy\Df{\length}{1}{\clazy})\sigma_{\mathit{\length}}
 \\
\Lanv{9}{\pa} &= \clazy \bcar \Df{\length}{1}\epsilon
\end{align*}

In summary, the equations generated during liveness analysis are:
\begin{enumerate}
\item   For    each   function   $\mathit{f}$,    equations   defining
\Df{f}{i} for use by \Lfonly$_{\mathit f}$.
\item  For  each   function  $\mathit{f}$, an  equation  defining
the summary demand  $\sigma_{\mathit f}$ on $e_f$.
\item For each function $\mathit{f}$ (including $\mainpgm$ for
  $e_\mainpgm$)
an equation defining
  liveness at each GC point of $e_{\mathit f}$.
\end{enumerate}
\subsection{Solving liveness equations---the grammar
interpretation}\label{sec:grammar-formulation}
The  equations above can now be re-interpreted as a
context-free grammar (CFG)  on the alphabet $\lbrace\acar, \acdr,
\bcar, \bcdr, \clazy\rbrace$.  Let  \var{$X$} denote the non-terminal
for
a variable  $X$ occurring on  the LHS of the  equations generated
from the analysis.  We can  think of the resulting productions as
being associated with several  grammars, one for each non-terminal
\var{\Lanv{i}{x}} regarded as a start symbol.  As an example, the
grammar    for   \var{\Lanv{1}{\xl}}   comprises    the   following
productions:
 \begin{eqnarray*}
\var{\Lanv{1}{\xl}}  &\rightarrow&   \clazy\sigma_{\length} \mid \acdr
\var{\Df
  {\length}{1}}{\clazy\sigma_{\length}} \mid
  \clazy\var{\Df{\length}{1}}{\clazy\sigma_{\length}} \\
  \var{\Df{\length}{1}} &\rightarrow& \clazy \mid
  \acdr\var{\Df{\length}{1}}\clazy
       \mid \clazy\var{\Df{\length}{1}}{\clazy}\\
\langle {\sigma_{\length}} \rangle
&\rightarrow&
\epsilon  \mid \clazy\langle{\sigma_{\length}}\rangle
\end{eqnarray*}
Other equations can be converted similarly.  The language generated by
\var{\Lanv{i}{x}},  denoted $\mathscr{L}(\var{\Lanv{i}{x}}) $,  is the
desired  solution of  \Lanv{i}{x}.  However,  recall from  our earlier
discussion that the decision problem  that we are interested in during
garbage collection at a program point $\pi_i$ is:
\begin{quote}
Let  $x.\alpha$ be  a forward  access  path consisting  only of  edges
\acar\    and   \acdr\    (but    not   \bcar, \bcdr\ or \clazy).     Let
$\mathscr{L}(\var{\Lanv{i}{x}}) \stackrel{*}{\hookrightarrow} \sigma$,
where $\sigma$ consists  of forward paths only. Then  does $\alpha \in
\sigma$?
\end{quote}

We now model the above problem  as one of deciding the membership of a
context-free  grammar  augmented  with  a fixed  set  of  unrestricted
productions.

\begin{definition}\label{def:specialgrammar}
Consider a form of  grammar $(N,T, p_1\cup p_2,S)$ in which $N$ is a set of
non-terminals, $T = \{\acar, \acdr, \bcar, \bcdr, \clazy, \$\}$, $p_1$
is a set of context-free production that contains the distinguished
production $S \rightarrow \alpha\$$, $\alpha$ is a string of grammar
symbols that does not contain $S$, and $p_2$ is the fixed set of
unrestricted productions $\bcar\acar \rightarrow \epsilon$,
$\bcdr\acdr \rightarrow \epsilon$, $\clazy\acar \rightarrow \clazy$,
$\clazy\acdr \rightarrow \clazy$, and $\clazy\$ \rightarrow
\epsilon$.
\end{definition}

From Sections \ref{sec:liveness}  and \ref{sec:computing}, it is clear
that the results of liveness analysis of any program can be modeled by
the kind  of grammar described above. The  following proposition shows
that the converse also holds.
\begin{proposition}
Given    a     grammar    $G$    of    the     form    described    in
Definition~\ref{def:specialgrammar},  it is  possible  to construct  a
program $p$  with program points  $\pi_i$ and variables $x$  such that
the liveness  analysis of $p$  is same as  $G$ except for a  change in
non-terminal names which now have the form \var{\Lanv{i}{x}}.
\end{proposition}
We now show that the decision problem required at garbage collection
time is undecidable.

\begin{lemma}
Consider a grammar $G$ of the kind  described in
Definition~\ref{def:specialgrammar} and a forward access path $\alpha$
consisting
of symbols \acar\ and \acdr\  only. The decision problem $\alpha \in
\mathscr{L}(G)$ is undecidable.
\end{lemma}

\comment{Proof:}


We circumvent  the problem by over-approximating the  CFG  by
non-deterministic finite state automata (NFA). The NFA are then
simplified using the $\hookrightarrow$ rules. Finally the simplified
NFAs are converted to DFAs.

%------------------------------------------------------------%
\subsubsection{From CFGs to GC-ready DFAs}
\label{sec:NFA-approx}
 We use  the algorithm by Mohri  and Nederhof~\cite{mohri00regular} to
 approximate a CFG to a {\em strongly regular\/} grammar.  For
 example, the grammar
 fragment  for  the  non-terminal  $\var{\Df{\length}{1}}$  after  the
 Mohri-Nederhof transformation is:
 \begin{eqnarray*}
   \var{\Df{\length}{1}} &\rightarrow& \clazy\var{\Df{\length}{1'}} \mid
   \acdr\var{\Df{\length}{1}}
   \mid \clazy\var{\Df{\length}{1}}\\
   \var{\Df{\length}{1'}} &\rightarrow& \clazy\var{\Df{\length}{1'}}
   \mid \epsilon
 \end{eqnarray*}

The strongly regular grammar is converted into a set of
NFAs,   one  for   each  $\var{\Lanv{i}{x}}$.   The  $\hookrightarrow$
simplification  is now  done  on the  NFAs  by repeatedly  introducing
$\epsilon$  edges  to  bypass  pairs  of  consecutive  edges  labelled
\bcar\acar\ or \bcdr\acdr\  and constructing the
$\epsilon$-closure. Since the $\epsilon$-closure step may create more
adjacent \bcar\acar\ and \bcdr\acdr\  edges, the simplification is
 continued  till a  fixed point is  reached. The algorithm  for this
appears  in~\cite{asati14lgc},   Figure[].  Finally  we   replace  the
$\clazy$  edges  by  $\epsilon$  and determinize  the  automaton  thus
formed.

Figure~\ref{fig:example-automata}(a)  shows the  NFA that  is obtained
from the  grammar for  \var{\Df{\length}{1'}}.  The  NFA is
simplified by replacing the edges labeled \clazy\ by $\epsilon$. While
checking for liveness during GC, a forward access-path traversing such
an edge  will be accepted  only if it  can reach a final  state.  This
effectively implements  the $\hookrightarrow$ simplification  rule for
\clazy for forward paths. The resulting automaton, made deterministic,
is shown in Figure~\ref{fig:example-automata}(b). This expectedly says
that  given  a demand  $\epsilon$  on  \length,  the liveness  of  its
argument             is             $\acdr^{*}$.            Similarly,
Figure~\ref{fig:example-automata}(c)    shows   the    automaton   for
\var{\Lanv{9}{\pa}}. The simplified automaton, shown in
Figure~\ref{fig:example-automata}(d) does not accept any forward
paths. This reflects the lazy nature of our language. \length\  does not
evaluate its elements and thus \pa\ is dead at $\pi_9$.

\begin{figure}[t!]
\hspace*{1cm}{\epsfig{file=example-automata.eps, height=2.75cm}}
\caption{(a) The grammar rules for $\langle \Df{\length}{1'}\rangle$
converted into
  an automaton, and simplified in   (b). (c) and (d) show the automata
  for   \Lanv{9}{\pa}.}\label{fig:example-automata}
\figrule
\end{figure}


\section{The Garbage Collection Scheme}
% GC assumptions
\cred{Introduction}

Our experimental setup consists of  an interpreter for our language, a
liveness  analyzer, and  a single  generation copying  collector.  The
garbage collector  can be  easily configured to  work on the  basis of
reachability (RGC  mode) or use  the liveness DFAs (LGC  mode).  While
exploring the  activation record of a  function, LGC uses  the DFAs at
the latest program  point traversed in the function's  body.  Thus, if
LGC is invoked  at the program point $\pi$, it  would use the liveness
DFAs  at  $\pi$ itself  for  exploring the  root  set  of the  current
activation record. For  any other activation record in  the stack, say
for a function ${\mathit f}$  calling ${\mathit g}$ in the call-chain,
the evaluation  point in ${\mathit f}$  which resulted in  the call to
${\mathit g}$ would be used.

While exploring the heap, the  GC may encounter closures. In RGC mode,
the  closure will  be explored  as  usual. However,  \cred{  as
  mentioned earlier}, our liveness analysis predicts the liveness of
data in its evaluated form. The absence of liveness information of
closures presents challenges for LGC.


\subsection{A liveness-based garbage collection scheme}
We shall call  a unit allocatable memory as a {\em  cell}.  A cell can
hold  a  basic  value   ($\mathit{bas}$),  the  constructor  \CONS\  $(\mathit{cons~
arg_1~arg_2}$) or   a closure.  The closure, in  turn, can be  one of
($\mathit{unop~arg_1}$),  ($\mathit{ binop~arg_1~arg_2}$)  and  function  application
($\mathit{app~arg_1~arg_2})$.  Here each $\mathit{ arg_i}$ is a reference to another heap
cell or terminated through the function $\mathit{makenull}$. 

\begin{figure*}[t!]
\label{fig:algos}
\hfill
\subfigure[]{\includegraphics[width=.30\linewidth]{lgc-scheme-1}}
\hfill
\subfigure[]{\includegraphics[width=.40\linewidth]{lgc-scheme-2}}
\hfill
\caption{(a) LGC with closure collection based on liveness.  (b)
  LGC with reachability-based closure collection.}
    
\end{figure*}
 
  
Figure~\ref{fig:algos}  shows   a  liveness-based  garbage  collection
scheme.   The algorithm  assumes that  checking for  liveness involves
traversing the  liveness-DFAs stored as  a table and does  not mention
this explicitly.  Starting  with the root set, each  live reference is
explored   using  $\mathit{  lgcCopy}$.    Copying  \CONS\   cells  is
simple---it  just involves  copying  the cell  itself and  recursively
copying the \CAR\ and  the \CDR\ fields.  Liveness-based collection of
closures  is more  complicated since  our analysis  only  computes the
liveness of evaluated objects and not closures.

Let   us  consider   determining  the   liveness  of   a   closure  at
runtime. Assume  that the closure  $(app~f~y)$ has a  reference from
the root  set variable $x$. If $x$  is live,  then we  know that the
cell $(app~f~y)$ has  to be copied. However, to find out  the
liveness of $y$, could find out the program point which triggered the
evaluation of $(app~f~y)$ and pick up the liveness of $y$ from this
program point. However, this is only possible if the activation record
of the function which created this closure is still on the activation
stack. This does not always happen since a function could pack a
closure in a \CONS\ cell and return the \CONS\ cell.

An  alternative  could  be   to  carry  extra  information  with  each
closure---the {\em free variables in the closure and the program point
  at  which  it  was  evaluated}.   This  gives  us  precise  liveness
information  of the  free  variables  of a  closure  but involves  the
overhead of  updating the program point  information during execution.
To avoid this  overhead we go for a safe  approximation by storing the
{\em creation  point of the closure}  and using it  to check liveness.
This is safe  since the liveness at the creation  point of the closure
dominates all possible evaluation points  of the closure, but it might
be imprecise  as the actual  liveness might be  way less than  what is
found at the creation point.

Note that  when a reachability-based  collector visits a cell  for the
first time,  it is  marked.  Explorations starting  from the  cell are
curtailed if the cell is  visited subsequently due to sharing. This is
because the collector can assume  that the region of the heap starting
from  the  cell has  already  been  explored.  Since a  liveness-based
collector may  reach the cell each  time with a  different state, this
curtailment cannot  be done in  the case of  LGC. This is  an inherent
drawback  of a  liveness-based collector.   This can  be  mitigated by
maintaining a list of liveness  automata states that a particular heap
cell was traversed with and  ensuring that we do not repeat traversals
if we reach the cell in one of these states.

\subsection{Liveness-based collector with reachability-based closure
  collection}  Since the first  algorithm has  significant bookkeeping
overheads,  we have  a  second GC  scheme  that not  use liveness  for
copying closures.  In this  scheme, the liveness automata is consulted
only for \CONS\ cells; we assume everything under a closure to be live
and copy everything  that is reachable. 
 
A consequence of copying everything under a closure is that we can end
up trying to copy things which are not live and corrupt the heap. This
can happen if a \CONS\  cell copied using liveness during a collection
subsequently  becomes   part  of  a  closure.    During  a  subsequent
collection,  the  collector  may  try  to copy  everything  under  the
\CONS\ cell thus copying heap  locations which might not be valid.  To
avoid  this, we  should ensure  that any  non-live references  are not
carried across  GC's.  We  do this  by scanning the  live part  of the
buffer after every  GC and setting to null any  reference which is not
live. This  is correct  because any reference  which is dead  during a
collection will not be dereferenced any further.


An advantage of  using reachablity for closures it  avoids the problem
of  multiple  explorations  from  the  same  cell.  This  is  done  by
maintaining  a flag in  each cell  to indicate  whether it  was copied
using  reachability. 


\subsection{Garbage collection for references on print stack}
In lazy  languages the evaluation of  closures is driven  by the print
function~\cite{Jones87}.  Printing of  atomic values is
simple  and  does  not  trigger  a garbage  collection.   In  case  of
\CONS\ cells, the \CAR\ is first  printed followed by \CDR.  In a lazy
language, both \CAR\ and \CDR\ could be closures requiring evaluation,
and this  may trigger  a garbage collection.   When this  happens.  we
have to consider  any references that might be on  the print stack and
copy them. We extend the liveness analysis to the print function and
use its results during garbage collection.

%% Liveness automata  does not exist for  the print function  as it is
%% not part of the executing  program.  Since the print function makes
%% everything  reachable  from a  cell  live  we  can copy  all  cells
%% reachable from the reference.


%% Consider  the case  where a  \CONS\ cells  is being  printed  and a
%% garbage collection  is triggered when  printing the \CDR\  part. In
%% this case, since  the refernce to the \CONS\ cells  is still on the
%% print stack both the \CAR\ and the \CDR\ parts are reachable but we
%% know that the \CAR\ part has already been printed and can be safely
%% garbage collected. A liveness  analysis of the print function would
%% have given  the same information.   In our implementation  we mimic
%% the  behavior of the  liveness analysis  by using  a flag  in every
%% \CONS\ cell which  says whether the \CAR\ part  has been printed or
%% not. For any \CONS\ cell if  the \CAR\ part has not been printed, a
%% reference  to the  \CAR\ part  will also  be on  the stack  but the
%% \CDR\ part has to be explicitly copied.  In case the \CAR\ part has
%% been printed,  then a reference  to the \CDR\  part will be  on the
%% stack and  since the \CAR\ part  is not required anymore  we do not
%% copy anything.  This  optimization subsumes tail call optimization.
%% Again, experimental results show that this optimization minimizes a
%% lot of copying.  \cred{Very sketchy: Make another attempt to write}
%% \hrule

%% \comment{ Unlike  eager languages where  GC happens only  at \CONS~
%%   points or at function calls,  every \LET~ point is a potential GC
%%   point.  The number of automata that needs to be stored is reduced
%%   by sharing  automata along all  the program points along  an {\em
%%   E-path}.

%% By the time the program starts executing, the liveness automata are
%% created  and stored  in a  file.  This  is re-used  every  time the
%% program executes.   An important difference in GC  of lazy language
%% compared to an  eager language is the point  of garbage collection.
%% In case  of eager languages  only \CONS~ points and  function calls
%% are the  GC points. In a  lazy language every \LET~  statement is a
%% potential  GC  point. Hence  we  need  to  store liveness  automata
%% corresponding to  every \LET~ statement in the  program. This leads
%% to an  explosion in  the number of  automata. From  our observation
%% that  along  an {\em  E-path}  all  program  points have  the  same
%% liveness, we  can reduce the  number of automata state  required by
%% sharing  the same  automata for  all program  points along  an {\em
%% E-path}.} In the  example program, we only need  to create automata
%% for program  points \ldots, since  they are the only  distinct {\em
%% E-path}s in the program.






\section{Results}
%% \comment{To demonstrate the effectiveness of liveness-based garbage collection,
%% we  have  built a  prototype  consisting  of  an interpreter  for  our
%% language,  a  liveness  analyzer  and  a copying  collector  that  can
%% optionally use the results of liveness analysis for marking instead of
%% reachability. When the collector uses liveness for marking, we call it
%% a  {\em liveness-based  collector} (LGC),  else we  use the  term {\em
%%   reachability-based  collector}  (RGC).   The  copying  collector  is
%% neither incremental nor generational.  As a consequence, any cell that
%% becomes unreachable or  dead is assuredly collected in  the next round
%% of garbage collection.}

To test the utility of our method we have implemented a garbage-collector 
for a simple scheme-like first order language. It consists of an interpreter, 
liveness analyzer and a garbage-collector that can optionally use use reachability or liveness. 
Our benchmark consists of programs from nofib suite and some external programs manually converted to ANF. 

%% \comment{
%% While exploring the activation record of a function, LGC uses the DFAs
%% at  the latest  program  point traversed  in the function's body.
%% Thus, if  LGC
%% is invoked  at the program point   $\pi$,  it would  use  the
%% liveness  DFAs at  $\pi$ itself for exploring the root set of the
%% current activation record. For any other activation record in the
%% stack, say for a function ${\mathit f}$ calling ${\mathit g}$ in the
%% call-chain,
%% the DFAs at the point of call to  ${\mathit g}$ would be used.}

The process of liveness-based garbage collection involves going through 
each activation record on the stack and exploring each variable in the 
current activation record (root set). The liveness of each variable is 
determined using the program point and the variable name. The liveness 
of all variables are stored as DFAs. These DFAs are then encoded as a 
table. A variable is live only if an entry is found in this table.
All cells that are live are copied to the live semi-space. In case of 
\CONS\ cells the \CAR\ and \CDR\ pointers are chased and if they are 
live the copied \CONS\ cell will get the updated addresses of its 
\CAR\ and \CDR\ fields.


%% \comment{
%% Let $\dfa{\pi}{x}$  denote the DFA  for the variable  and program
%% point  pair ($x$,  $\pi$).  We  assume that  there is  a function
%% $\mathsf{initial}(\dfa{\pi}{x})$  returning the initial  state of
%% $\dfa{\pi}{x}$.
%% Considering   a   DFA   as   a   table,
%% $\dfa{\pi}{x}\mathit{(q,sym)}$  returns the  next  state for  the
%% state $q$ and the  symbol $\mathit{sym}$, where $\mathit{sym}$ is
%% \acar\      or     \acdr.       We      shall     also      write
%% $\dfa{\pi}{x}\mathit{(q,sym)\mathsf{?}}$    for    a    predicate
%% indicating  whether there  is a  transition from  $q$ on
%% $\mathit{sym}$.  The LGC action to chase the root variable $x$ at
%% $\pi$ can be described as  follows: If $\mathscr{L}(\dfa{\pi}{x})$ is
%% empty,  then  nothing  needs  to  be  done.   Otherwise  we  call
%% $\mathsf{copy}(\dfa{\pi}{x},\mathsf{initial}(\dfa{\pi}{x}),    x)$
%% in   Figure~\ref{algo:LGC-main-loop}  and  assign   the  returned
%% pointer to $x$.
%% The function $\mathsf{move\_to\_tospace}(x)$  copies the value of
%% $x$  in the  other semi-space  and returns  the new  address.  It
%% hides  details such as  returning the  forwarding pointer  if the
%% value  of $x$  is  already copied,  and  creating the  forwarding
%% pointer otherwise.}

\subsection{Copying closure using liveness vs Copying closures using reachability}
To compare the effect of using reachability to copy closures instead of liveness we take a couple of programs from our benchmark suite and execute them with 
both. As shown in figure-add-figure-here, the number of garbage collections 
in both cases are same but the time taken varies largely. Both these observations can be explained as follows, since we approximate the liveness of closure to 
the liveness at its creation point, the liveness we get is usually grossly over approximated. 
Using this liveness may not give too much of a benefit compared to reachability as nearly everything would be live at its point of creation. 
As mentioned in section-ref, using liveness for closures may entail multiple traversals over the same structure consuming more time. 
Therefore we believe that using reachability for closures is a sweet spot in terms of performance for liveness based garbage collectors. 
%%
%% \comment{
%% The graphs  in Figure~\ref{fig:memory-usage} show the  number of cells
%% in the heap over time for RGC and LGC---here time is measured in terms
%% of the number of cons cells allocated. In addition, they also show the
%% number of  reachable cells and the  number of cells  that are actually
%% live\footnote{This   is  statically   approximated  by   our  liveness
%%   analysis}.  Since  the programs have  different memory requirements,
%% we have tuned the size of heap for each program to ensure a reasonable
%% number of  collections. An invocation  of RGC decreases the  number of
%% cells  in heap  till  it touches  the  curve of  reachable cells.   An
%% invocation of LGC decreases the number  of heap cells to no lower than
%% the curve of live cells.

%% To construct the  reachable and live curves, we  record for every cell
%% its creation time (\CreateTime), its last use time (\UseTime), and the
%% earliest time  when the  cell becomes unreachable  and can  be garbage
%% collected (\CollTime).  For accurate recording of  \CollTime, we force
%% frequent invocations  of a reachability based collector  in a separate
%% run.     A    cell   is    live    at    time   \mbox{\exectime}    if
%% \mbox{$\mbox{\CreateTime}       \leq       \mbox{\exectime}       \leq
%%   \mbox{\UseTime}$}.       If      \mbox{$\mbox{\CreateTime}      \leq
%%   \mbox{\exectime} \leq \mbox{\CollTime}$}, it is reachable.

%% The benchmark programs are drawn from the {\tt no-fib} suite and other
%% sources and have been  manually converted to  ANF.  All  graphs  except
%%  {\tt  fibheap}  show  fewer  garbage
%% collector  invocations for  LGC. The  number  of reachable  cells in
%% {\tt
%%   fibheap} increases  initially, comes  down before the  heap becomes
%% gets
%% full and then  continues to remains low.  This results in  a single
%% garbage
%% collection in both RGC and LGC.  The graphs also show the precision of
%% our liveness analysis.  For  all programs  except  {\tt nperms} and
%% {\tt lambda} (Figure~\ref{fig:memory-usage-lambda}),  LGC
%% manages to collect a good portion of the data that are not live.}
%%


\subsection{Results}
%% \begin{figure*}[t]
%% \centering
%% \begin{boxedminipage}{.75\textwidth}
%% {\bf function} $\mathsf{copy}$($\mathsf{dfa}$, $q$, $x$)\\
%% \noindent \hspace*{.35cm}$y \leftarrow \mathsf{move\_to\_tospace}(x)$\\
%% \noindent \hspace*{.35cm}{\bf if} $x$.{\bf tag} $\neq$ \CONS\ {\bf
%%   then} {\bf skip}\\
%% \noindent \hspace*{.25cm}  {\bf else} {\bf if}
%% $\mathsf{dfa}(q,0)\mathsf{?}$ {\bf then}   $y.\CAR =
%% \mathsf{copy}(\mathsf{dfa}, \mathsf{dfa}(q,0),x.\CAR)$\\
%% \noindent \hspace*{0.95cm} {\bf if}  $\mathsf{dfa}(q, 1)\mathsf{?}$
%% {\bf then}  $y.\CDR = \mathsf{copy}(\mathsf{dfa},
%% \mathsf{dfa}(q,1),x.\CDR)$\\
%% \noindent \hspace*{.35cm}{\bf return} $y$
%% \end{boxedminipage}
%% \caption{Function       for       copying       a       root       set
%%   variable.}\label{algo:LGC-main-loop} \figrule

%% \end{figure*}



%% \begin{figure}[p]
%% \begin{tabular}{@{}c@{}c@{}}
%% \hskip -4mm{\epsfig{file=sudoku.eps, height=4cm, width=6.10cm}}
%% &
%% {\epsfig{file=lcss.eps, height=4cm, width=6.10cm}}
%% \\
%% \hskip -4mm{\epsfig{file=gc_bench.eps, height=4cm, width=6.10cm}}
%%  &
%% {\epsfig{file=nperm.eps, height=4cm, width=6.10cm}}
%% \\
%% \hskip -4mm{\epsfig{file=fibheap.eps, height=4cm, width=6.10cm}}
%% &
%% {\epsfig{file=knightstour.eps, height=4cm, width=6.10cm}}
%% \\
%% \hskip -4mm{\epsfig{file=nqueens.eps, height=4cm, width=6.10cm}}
%% &
%% {\epsfig{file=treejoin.eps, height=4cm, width=6.10cm}}
%% \end{tabular}
%%  \caption{Memory  usage of  programs. The blue and the red curves
%%   indicate the number of cons cells in the active semi-space for RGC
%% and
%%   LGC respectively. The black curve represents the number of reachable
%%   cells and the grey curve represents the number of cells that are
%%   actually live (of which liveness analysis does  a static
%%   approximation). x-axis is the time measured in number of cons-cells
%%   allocated. y-axis is the number of cons-cells.}
%% \label{fig:memory-usage} \figrule
%% \end{figure}
The table in Figure~\ref{fig:experimental-results} shows that LGC improves the 
garbage collection time and efficiency for most of the programs. 
Even for programs where the time taken is more it is very competitive. 
As the number of garbage collections tend to be very large our graphs show only 
a window which is representative of the behavior for that particular benchmark. 
In all the programs we can see that the curve corresponding to liveness regularly 
dips below the reachability curve. 

One area of concern is the huge gap between the actual liveness and the 
liveness percieved by our collector. In case of LGC for eager languages the 
gap was very narrow and almost touched the actual liveness curve. 
In case of lazy languages, due to using reachability for copying we end up copying 
more cells. Any data which is part of closure irrespective of whether it is live 
or not gets copied. Thus many non-live cells get copied during garbage collection 
increasing the drag time.
Our experiments have shown that unless we use the exact execution point for 
closures bringing down the number of non-live cells copied in closures cannot be reduced.
Using the approximated liveness (liveness at the creation point) does not give benefits in terms of non-live
 cells inside closures as everything would be live at the point of creation.
%% \comment{
%% The  increased effectiveness  of LGC  over RGC  is also  shown  in the
%% tables  in  Figure~\ref{fig:experimental-results}.   The  first  table
%% provides  statistics  regarding the  analysis  itself.  The number  of
%% states and the analysis times  are within tolerable limits. Precision
%% of analysis refers to the percentage of dead cells that is collected by
%% LGC, averaged over all invocations.
%% The second table shows garbage  collection statistics for RGC and LGC.
%% LGC  collects larger garbage  per invocation,  drags cells  for lesser
%% time and requires a smaller heap size ({\em MinHeap}) for program to
%% run
%% in comparison with  RGC.

%% There are a couple of  issues of concern.  The garbage collection time
%% is larger in the case of LGC for some programs. The reason is that the
%% cost of consulting the liveness DFA may outweigh the combined benefits
%% of  fewer   garbage  collections   and  fewer  markings   per  garbage
%% collection.   The  other issue  is  illustrated  by  the program  {\tt
%%   lambda}    (memory   usage  shown   separately    in
%% Figure~\ref{fig:memory-usage-lambda}).  As can be seen from the table
%% in Figure~\ref{fig:experimental-results}, the  number of
%% touched cells\footnote{These  are the  cells which are  visited during
%%   marking phase, often  more than once due to  sharing.} in this
%% example is much higher for
%% LGC.  This  increase is due to  excessive sharing among  heap nodes in
%% this program.  Note  that a node re-visited because  of sharing is not
%% explored  any   further  during  a  RGC   collection.   However,  this
%% curtailment cannot happen in  LGC because of the possibility that the
%% node,  re-visited in a  different liveness  state, may  mark a  set of
%% cells different from the earlier visit.
%% }
\begin{figure*}[t]
\footnotesize

\centering

\scalebox{.9}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
{Program} & {\sf sudoku} & {\sf lcss} & {\sf gc\_bench} &  {\sf
  nperm} &  {\sf fibheap} & {\sf knightstour} &
{\sf treejoin} & {\sf nqueens} & {\sf lambda}\\
\hline
\hline
{Time (msec)}&106.56 &2.19 &0.32 &1.16 &2.4 &3.05 &2.61 &0.71&8.68 \\
{DFA size} &4206 &726 &258 &526 &675 &922 &737 &241 & 694\\
{Precision(\%)} &98.4&98.8&99.9&87.1&100&94.3&99.6&98.8&79.7\\
\hline
\end{tabular}}



\centerline{(a)}

\medskip

\scalebox{0.87}{\begin{tabular}{| c | r | r | r | r | r | r  |  r | r |
r | r | r | r | r |}
\hline
    & \multicolumn{2}{c|}{\# Collected} & \multicolumn{2}{c|}{\#
Touched}
  & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{} &
\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{GC time} \\



                            &   \multicolumn{2}{c|}{cells per GC}
                            &   \multicolumn{2}{c|}{cells per GC}
                            &   \multicolumn{2}{c|}{\#GCs}
                            &   \multicolumn{2}{c|}{MinHeap}
                            &   \multicolumn{2}{c|}{Avg. Drag}
                            &   \multicolumn{2}{c|}{(sec)} \\
\cline{2-13}
{Program}    &
RGC & LGC & RGC & LGC  & RGC & LGC  &   RGC & LGC & RGC & LGC & RGC &
LGC \\
\hline
\hline
    {\sf   sudoku}  &381 &1107 &1268 &281 &8 &3 & 1346  &338 &527 &5
&.007 & .034 \\
    {\sf  lcss } & 46522 &51101 &6216 &1363&8&7& 52301  &1701 &5147
&588 &.045 & .144 \\
    {\sf   gc\_bench}  &129179 &131067 &1894 &4&9&9& 131071   &6 &16970
 &4 &.086 & .075 \\
     {\sf  nperm}  & 47586  &174478 &201585 &60882&14&4& 202597  &37507
&171878 &76618 &1.406 & .9  \\
    {\sf  fibheap} &249502  &251525 &5555 &2997&1&1& 254520  &13558
&78720 &0 &.006 & .014  \\
    {\sf  knightstour}  &2593 &314564 &907502 &319299&1161&10&508225
&307092 &206729 &82112 &464.902 & 14.124  \\
    {\sf  treejoin} & 288666  &519943 &297570 &5547&2&1& 525488  &7150
&212653 &1954 &.356 & .217 \\
    {\sf   nqueens} & 283822 &1423226 &2133001 &584143&46&9& 1819579
&501093 &521826 &39465 &70.314 & 24.811 \\
    {\sf   lambda}  &205 & 556 &2072&90345 &23 &8&966 & 721  &303 &95
&.093 &2.49  \\
%    {\sf   fft }& &  & & &&&& ???? &???? &????? &????? &???? & ????
\\
%    {\sf  nboyer} & &  & &   & &  & & &  \\
%    {\sf   circsim?} & &  & &   & & & & &   \\
 \hline
\end{tabular}}


\centerline{(b)}
\vskip -2mm
\caption{Experimental results comparing RGC and LGC. Table (a) gives
  data related to liveness analysis, and (b) gives garbage collection
  data. The average drag time is measured in allocated cons cells.}
\label{fig:experimental-results}
\normalsize
\end{figure*}

%% \section{Collecting more garbage can never slow things down}
%% \label{sec:lgc-always-better}

%% Since garbage collection is  effectively asynchronous to the
%% allocator
%% thread, one might  worry as to how {\em  robust} our measurements
%% are.
%% For example,  while LGC would,  in general, collect more  garbage
%% than
%% RGC in the same heap state,  is it possible that on some programs,
%% LGC
%% might do  a larger number  of collections? We  prove a result  to
%% show
%% that   this  cannot   happen.    This  result   applies  to
%% classical
%% mark-and-sweep and copying garbage  collectors and, we hypothesize,
%% to
%% generational collectors too.

%% \begin{lemma}
%% For the  same mutator,  a liveness-based collector  can never  do
%% more
%% garbage collections than a reachability-based collector.
%% \end{lemma}

%% \begin{proof}Assume, as before,  that time is measured in terms of
%% the number of
%%   cons cells allocated. Let us run  two copies of the mutator with
%% the
%%   two    different   collectors    but    with   synchronous
%% memory
%%   allocations.   The   garbage    collections,   however,   happen
%%   asynchronously.

%%   To prove the lemma, it is enough to show the truth of the
%% following
%% statement: After
%%  every  LGC invocation,  the count of LGC invocations   is  no
%% greater
%% than  RGC invocations.   The  base case  holds  since the  first
%% invocations of both GCs happen at the same time.  Assume the
%% statement
%% to be true after $n$ invocations of LGC.  Since LGC copies a subset
%% of
%% reachable cells, its heap would contain no more cells than RGC heap
%% at
%% the end  of the $n$th invocation.   Thus either RGC  is invoked next
%% before
%% LGC, or LGC and RGC are both invoked next at the same time.  In
%% either
%% case, the statement holds after $n+1$ invocations of LGC.
%% \end{proof}



%% \begin{figure}[h]
%% \centerline{\epsfig{file=memory-usage.eps, width=12cm, height=15cm}}
%% \caption{Memory  usage of  programs. The  solid  lines represents
%%   number of  live cells and the  dashed lines show  the number of
%%   reachable cells.  The red and blue  lines represent invocations
%%   of RGC and  LGC---their heights indicate  the amount of memory
%%   copied.   {\color{red}{Caution:  Figures  are from  a  previous
%%       paper and are being used as placeholder}}}
%% \label{fig:memory-usage} \figrule
%% \end{figure}
%%\clearpage

\section{Related Work} Previous attempts to  increase  the space
efficiency
of functional programs by additional  reclamation of memory fall in two
broad categories. In the first,  the program itself is instrumented to
manage  reclamation and reallocation  without the  aid of  the garbage
collector.    Such    attempts   include:   sharing    analysis   based
reallocation~\cite{jones89compile},                       deforestation
techniques~\cite{wadler88deforest,gill93ashort,chitil99deforest},
methods  based  on   linear  logic~\cite{hofmann00linear}  and  region
analysis~\cite{tofte98region}.   Closer  to  our approach,  there  are
methods   that  enable   the   garbage  collector   to  collect   more
garbage~\cite{inoue88analysis,lee05static}   by
explicitly  nullifying  pointers  that  are not  live.   However,  the
nullification,  done   at  compile  time,   requires  sharing  (alias)
analysis.  Our  method, in contrast, does not  require sharing because
of the availability of the heap  itself at runtime.  To the best of our
knowledge, this is the first  attempt at liveness-based marking of the
heap during garbage collection.

%% \begin{figure}[t]
%% \centerline{\epsfig{file=lambda.eps, height=4cm, width=7cm}}
%%  \caption{Memory  usage and garbage collection pattern of  {\tt
%% lambda}.}
%% \label{fig:memory-usage-lambda} \figrule
%% \end{figure}

\section{Conclusions}
\label{sec:conclusion}
We  have  defined  a  notion  of liveness  on  structured  data;  this
generalizes classical liveness and strong liveness.  We started with a
general fully context-sensitive analysis  which we proved correct with
respect to  a minefield semantics  (this models the effect  of garbage
collection between every evaluation step).

To  avoid scalability  issues (and  to  avoid performing  part of  the
liveness computation at run time)  we defined an 0-CFA version of this
liveness analysis  in which  demands for function  $f$ at  all calling
contexts are conflated into  a single demand $\sigma_f$.  This enabled
us to treat the liveness equations symbolically obtaining context-free
grammars for liveness at each GC point (calls to user functions and to
$\CONS$).  These were then converted to DFAs for run-time consultation
by  the garbage collector.  Experiments confirm  the precision  of the
analysis.

To obtain performance figures we compared a reachability-based garbage
collector with a liveness-based  collector.  This showed a decrease in
the  number  of  GCs,   more  garbage  collected  per  invocation.   A
significant benefit of LGC is  that programs can run in smaller memory
when compared to  RGC. This is potentially useful  in situations where
memory  is  limited---as with  embedded  systems.  For  a majority  of
programs, the garbage collection times were reduced.

One issue we  highlighted was that while fewer  nodes were marked (and
more garbage collected), sometimes  cons cells could be visited and
traversed  multiple times  with different  sets of  liveness  paths to
explore.  Further work includes  improvements to the classical copying
collector to reduce the cost of this.


%Add graphs for memort usage LGC vs RGC
%Add graphs/table showing the GC times and other statistics

%Mention lower peak memory usage.
%Mention faster GC than full LGC and more garbage collected than RGC.
%Pitch it as sweet spot between full LGC and RGC.
%Future work
%Mention full LGC
%Mention k-liveness, use length example to show 1-liveness is not
sufficient.
%Mention mixed mode GC,
% - doing RGC most of the times and switching to LGC in case RGC fails
to collect any garbage
% - doing LGC most of the times but regularly checking the reachability
of the heap and
% switching to RGC if LGC almost equals RGC

\subsection{References}
\bibliography{fun_hra}{}
\bibliographystyle{abbrv}

%% Generated by bibtex from your ~.bib file.  Run latex,
%% then bibtex, then latex twice (to resolve references)
%% to create the ~.bbl file.  Insert that ~.bbl file into
%% the .tex source file and comment out
%% the command \texttt{{\char'134}thebibliography}.
%% % This next section command marks the start of
%% % Appendix B, and does not continue the present hierarchy
%% \section{More Help for the Hardy}
%% The sig-alternate.cls file itself is chock-full of succinct
%% and helpful comments.  If you consider yourself a moderately
%% experienced to expert user of \LaTeX, you may find reading
%% it useful but please remember not to change it.
%% %\balancecolumns % GM June 2007
%% % That's all folks!





%% % {\color{red}
%% %   (Note 3.\ includes $e_\mainpgm$ treated as the body of
%% %   $(\DEFINE\ ({\tt main})\ e_\mainpgm)$.)
%% % }




%% %% =======

%% %------------------------------------------------------------%
%% %\input{lazy-liveness}

\end{document}
